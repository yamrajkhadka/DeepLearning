# DeepLearning
Deep Learning Journey üöÄ This repo covers key concepts, code implementations, and projects on deep learning. It includes Jupyter notebooks, explanations, real-world case studies, and optimization techniques. Topics include neural networks, CNNs, RNNs, and model tuning. Explore and learn together! üöÄ
## Day 1: Perceptron - A Simple Neural Network Model
On Day 1 of my Deep Learning journey, I explored the **Perceptron**, a foundational algorithm for binary classification. In this session, I learned how to:

- Build a Perceptron model from scratch.
- Visualize decision boundaries.
- Train a Perceptron on a dataset.
- Evaluate model accuracy.
- Understand the Perceptron loss function.
- Identify the limitations of the Perceptron.

---

### Key Learnings

#### 1. What is a Perceptron?
- A **Perceptron** is a single-layer neural network that performs binary classification.
- Unlike Perceptrons, **Neurons** in multi-layer networks are part of more complex models with non-linear activation functions.

#### Key Difference:
A Perceptron is a simplified mathematical model of a neuron used in machine learning, while a Human Neuron is a complex biological cell involved in brain functions.

![Screenshot 2025-02-05 005546](https://github.com/user-attachments/assets/6b4653ff-476e-4019-8f76-6a5128fb44c5)
---
#### 2. Building and Visualizing a Perceptron
I implemented a **Perceptron** in Python and visualized its decision boundary between two classes.
![image](https://github.com/user-attachments/assets/5d8a4a5c-cd6c-4b1f-9f94-54bedbc220b6)
![image](https://github.com/user-attachments/assets/b16f620c-8a1e-4b56-ad33-00183b60b7f3)

#### 3.Limitation of perceptron
Inability to Solve Non-Linearly Separable Problems. The Perceptron struggles with datasets where classes cannot be separated by a straight line. A classic example is the XOR problem, which it cannot solve due to its linear decision boundary. 
!![image](https://github.com/user-attachments/assets/e8e57bf7-6078-4f2c-8eac-9da87b173cfb)




## üöÄ Day 2: Multi-Layer Perceptron (MLP) & Forward Propagation:

Today, I explored **Multi-Layer Perceptrons (MLP)** and **Forward Propagation**, key concepts in deep learning. Below is a summary of my learning with **visualizations, intuition, and code implementation**.  

---

### üìå What is Multi-Layer Perceptron (MLP)?  
MLP is a type of artificial neural network consisting of multiple layers:  
‚úî **Input Layer** ‚Äì Accepts the raw input data  
‚úî **Hidden Layer(s)** ‚Äì Applies transformations using weights & activation functions  
‚úî **Output Layer** ‚Äì Produces the final prediction  

MLP intution:
![image](https://github.com/user-attachments/assets/80bd9c1d-3b8b-45ec-b1d4-332057d34b1f)

MLP visualization in tensorflowbackground:
![image](https://github.com/user-attachments/assets/570624b1-ae35-42c1-b3c0-5873c3682666)



---

### üîÅ Understanding Forward Propagation  
Forward propagation is the process where input data moves through the network:  
1Ô∏è‚É£ Compute weighted sum of inputs in each neuron  
2Ô∏è‚É£ Apply activation function (e.g., Sigmoid, ReLU)  
3Ô∏è‚É£ Pass the result to the next layer until reaching the output  

![Forward Propagation Intuition](your_image_link_here)  

---

### üìù Implementation in Python (Without DL Framework)  
Here‚Äôs a simple **NumPy implementation** of forward propagation in an MLP:  

```python
import numpy as np

# Activation function: Sigmoid
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Forward Propagation
def forward_propagation(X, W1, b1, W2, b2):
    Z1 = np.dot(W1, X) + b1
    A1 = sigmoid(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    return A2  # Final output

# Example Input
X = np.array([[0.5], [0.2]])  

# Weights & Biases
W1 = np.array([[0.3, 0.6], [0.1, 0.4]])  # 2 neurons, 2 inputs
b1 = np.array([[0.1], [0.2]])

W2 = np.array([[0.5, 0.7]])  # 1 neuron, 2 inputs
b2 = np.array([[0.3]])

# Forward Propagation Execution
output = forward_propagation(X, W1, b1, W2, b2)
print("Final Output:", output)




